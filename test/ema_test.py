

import argparse
import torch
import diffusers
import numpy as np

from models.sd3_light import LightSD3Pipeline


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_dir", type=str, default="/inspire/hdd/project/chineseculture/public/yuxuan/diffusion-pipe/outputs/sd3_light_pretrain/test1/20251215_09-42-42/step20")
    parser.add_argument("--ema_shadow", type=str, default="/inspire/hdd/project/chineseculture/public/yuxuan/diffusion-pipe/outputs/sd3_light_pretrain/test1/20251215_09-42-42/step20/ema_shadow.pt")
    parser.add_argument("--expected_blocks", type=int, default=15)
    args = parser.parse_args()

    print("=" * 80)
    print("[1] Load pipeline via LightSD3Pipeline.load_from_pretrained")
    print("=" * 80)

    pipe = LightSD3Pipeline.load_from_pretrained(
        model_dir=args.model_dir,
        dtype=torch.float32,
        extra_model_config={"num_layers": args.expected_blocks},
    )

    transformer = pipe.transformer

    # ------------------------------------------------------------------
    # 1. éªŒè¯ transformer block æ•°
    # ------------------------------------------------------------------
    print("\n[CHECK-1] Transformer block count")

    num_blocks = len(transformer.transformer_blocks)
    print(f"Expected blocks: {args.expected_blocks}")
    print(f"Actual blocks:   {num_blocks}")

    assert num_blocks == args.expected_blocks, (
        f"âŒ Block count mismatch: expected {args.expected_blocks}, got {num_blocks}"
    )
    print("âœ… Block count correct")

    # ------------------------------------------------------------------
    # 2. åˆ—å‡ºæ‰€æœ‰ trainable parameters
    # ------------------------------------------------------------------
    print("\n[CHECK-2] Trainable parameters (requires_grad=True)")

    trainable = {
        name: p
        for name, p in transformer.named_parameters()
        if p.requires_grad
    }

    print(f"Trainable parameter count: {len(trainable)}")
    print("First 20 trainable parameters:")
    for i, k in enumerate(trainable.keys()):
        if i >= 20:
            break
        print(" ", k)

    # ------------------------------------------------------------------
    # 3. pos_embed æ˜¯ parameter è¿˜æ˜¯ bufferï¼Ÿ
    # ------------------------------------------------------------------
    print("\n[CHECK-3] pos_embed inspection (SD3-correct)")

    pos_params = [
        name for name, _ in transformer.named_parameters()
        if name.startswith("pos_embed.")
    ]

    pos_buffers = [
        name for name, _ in transformer.named_buffers()
        if name.startswith("pos_embed.")
    ]

    print("pos_embed parameters:", pos_params)
    print("pos_embed buffers:", pos_buffers)

    # å¿…é¡»åŒ…å«å¯è®­ç»ƒçš„ projection
    assert "pos_embed.proj.weight" in pos_params
    assert "pos_embed.proj.bias" in pos_params

    # å¿…é¡»åŒ…å« buffer positional grid
    assert "pos_embed.pos_embed" in pos_buffers

    print("âœ… pos_embed structure is SD3-correct")


    # ------------------------------------------------------------------
    # 4. åŠ è½½ EMA shadow
    # ------------------------------------------------------------------
    print("\n[CHECK-4] Load EMA shadow")

    ema_sd = torch.load(args.ema_shadow, map_location="cpu")

    assert isinstance(ema_sd, dict), "âŒ EMA shadow is not a dict"
    assert len(ema_sd) > 0, "âŒ EMA shadow is empty"

    print(f"EMA shadow entries: {len(ema_sd)}")
    print("First 20 EMA keys:")
    for i, k in enumerate(ema_sd.keys()):
        if i >= 20:
            break
        print(" ", k)

    # ------------------------------------------------------------------
    # 5. EMA keys âŠ† trainable parametersï¼Ÿ
    # ------------------------------------------------------------------
    print("\n[CHECK-5] EMA keys vs trainable parameters")

    trainable_keys = set(trainable.keys())
    ema_keys = set(ema_sd.keys())

    missing_in_model = ema_keys - trainable_keys
    missing_in_ema = trainable_keys - ema_keys

    print("EMA keys not in trainable parameters:", missing_in_model)
    print("Trainable parameters missing in EMA:", missing_in_ema)

    assert len(missing_in_model) == 0, "âŒ EMA contains non-trainable params"
    assert len(missing_in_ema) == 0, "âŒ Some trainable params not tracked by EMA"

    print("âœ… EMA tracking matches trainable parameters exactly")

    # ------------------------------------------------------------------
    # 6. éªŒè¯ load_ema_shadow æ˜¯å¦çœŸçš„æ›¿æ¢å‚æ•°
    # ------------------------------------------------------------------
    print("\n[CHECK-6] Verify EMA actually replaces weights")

    # éšæœºé€‰ 5 ä¸ªå‚æ•°åšæ•°å€¼å¯¹æ¯”
    test_keys = list(trainable_keys)[:5]

    before = {
        k: transformer.state_dict()[k].clone()
        for k in test_keys
    }

    missing, unexpected = pipe.load_ema_shadow(args.ema_shadow, strict=False)

    print("Missing keys:", missing)
    print("Unexpected keys:", unexpected)

    after = {
        k: transformer.state_dict()[k]
        for k in test_keys
    }

    diffs = []
    for k in test_keys:
        diff = (before[k] - after[k]).abs().mean().item()
        diffs.append(diff)
        print(f"Param {k}: mean |Î”| = {diff:.6e}")

    assert any(d > 0 for d in diffs), (
        "âŒ EMA load did not change any parameters"
    )

    print("âœ… EMA parameters successfully loaded and differ from normal weights")

    print("\n" + "=" * 80)
    print("ğŸ‰ ALL EMA / SD3-LIGHT VERIFICATIONS PASSED")
    print("=" * 80)


if __name__ == "__main__":
    main()
